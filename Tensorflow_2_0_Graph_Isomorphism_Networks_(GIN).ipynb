{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tensorflow 2.0 Graph Isomorphism Networks (GIN).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "private_outputs": true,
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/calciver/Graph-Isomorphism-Networks/blob/master/Tensorflow_2_0_Graph_Isomorphism_Networks_(GIN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxqLLb1FbZZa",
        "colab_type": "text"
      },
      "source": [
        "# Clone the repository"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mqqv_Kt2egel",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "ROOT_DIR = os.path.abspath(\"/content\")\n",
        "os.chdir(ROOT_DIR)\n",
        "!git clone https://github.com/calciver/Graph-Isomorphism-Networks.git\n",
        "\n",
        "WORK_DIR = os.path.abspath(\"/content/Graph-Isomorphism-Networks\")\n",
        "os.chdir(WORK_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nX0e4ee9bSp6",
        "colab_type": "text"
      },
      "source": [
        "# Install Tensorflow 2.0 and the associated requirements"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlAhldaJa-iK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q tensorflow-gpu==2.0.0-alpha0\n",
        "!pip install -r requirements.txt\n",
        "!unzip -q dataset.zip\n",
        "\n",
        "from __future__ import absolute_import, print_function, division, unicode_literals\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.python.ops import control_flow_util\n",
        "control_flow_util.ENABLE_CONTROL_FLOW_V2 = True\n",
        "\n",
        "print(tf.__version__)\n",
        "print(tf.executing_eagerly())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_tEsYY6De85H",
        "colab_type": "text"
      },
      "source": [
        "# Settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFkhiuPpfBKi",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title ### Choosing parameters for this Colaboratory runtime  { form-width: \"60%\", run: \"auto\"}\n",
        "#@markdown <br>Connect to a local or hosted Colaboratory runtime by clicking the **Connect** button at the top-right.\n",
        "\n",
        "#@markdown Following (Yanardag & Vishwanathan, 2015; Niepert et al., 2016), we perform 10-fold cross-
          validation with LIB-SVM (Chang & Lin, 2011). We report the average and standard deviation of
          validation accuracies across the 10 folds within the cross-validation. For all configurations, 5 GNN
          layers (including the input layer) are applied, and all MLPs have 2 layers. Batch normalization
          (Ioffe & Szegedy, 2015) is applied on every hidden layer. We use the Adam optimizer (Kingma
          & Ba, 2015) with initial learning rate 0.01 and decay the learning rate by 0.5 every 50 epochs.
          The hyper-parameters we tune for each dataset are: (1) the number of hidden units ∈ {16, 32} for
          bioinformatics graphs and 64 for social graphs; (2) the batch size ∈ {32, 128}; (3) the dropout ratio
          ∈ {0, 0.5} after the dense layer (Srivastava et al., 2014); (4) the number of epochs, i.e., a single epoch
          with the best cross-validation accuracy averaged over the 10 folds was selected. Note that due to the
          small dataset sizes, an alternative setting, where hyper-parameter selection is done using a validation
          set, is extremely unstable, e.g., for MUTAG, the validation set only contains 18 data points. We also
          report the training accuracy of different GNNs, where all the hyper-parameters were fixed across the
          datasets: 5 GNN layers (including the input layer), hidden units of size 64, minibatch of size 128,
          and 0.5 dropout ratio.",
        "\n",        
        "Dataset = \"REDDITBINARY\"  #@param [\"MUTAG\", \"COLLAB\", \"IMDBMULTI\",\"NCI1\",\"PTC\",\"REDDITMULTI5K\",\"IMDBBINARY\" ,\"PROTEINS\" , \"REDDITBINARY\"]\n",
        "Batch_Size = 32 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
        "Iterations_Per_Epoch = 50 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
        "Epochs = 350 #@param {type:\"slider\", min:1, max:1000, step:1}\n",
        "Learning_Rate = 0.01 #@param {type:\"slider\", min:0, max:1, step:0.0001}\n",
        "Seed = 0 #@param {type:\"slider\", min:0, max:10, step:1}\n",
        "Fold_idx = 0 #@param [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"] {type:\"raw\", allow-input: true}\n",
        "Num_Layers = 5 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "Num_MLP_Layers = 2 #@param {type:\"slider\", min:1, max:10, step:1}\n",
        "Hidden_dim = 64 #@param {type:\"slider\", min:1, max:300, step:1}\n",
        "Final_dropout = 0.5 #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "Graph_Pooling_Type = \"sum\"  #@param [\"sum\", \"mean\", \"average\"]\n",
        "Neighbor_Pooling_Type = \"sum\"  #@param [\"sum\", \"mean\", \"average\",\"max\"]\n",
        "Learn_Epsilon = \"True\"  #@param [\"True\", \"False\"]\n",
        "Degree_as_tag = \"True\"  #@param [\"True\", \"False\"]\n",
        "File_Name = \"output.txt\" #@param {type:\"string\"}\n",
        "\n",
        "\n",
        "import easydict\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "args = easydict.EasyDict({\n",
        "    \"dataset\": Dataset,\n",
        "    \"batch_size\": Batch_Size,\n",
        "    \"iters_per_epoch\": Iterations_Per_Epoch,\n",
        "    \"epochs\": Epochs,\n",
        "    \"lr\": Learning_Rate,\n",
        "    \"seed\": Seed,\n",
        "    \"fold_idx\": Fold_idx,\n",
        "    \"num_layers\": Num_Layers,\n",
        "    \"num_mlp_layers\": Num_MLP_Layers,\n",
        "    \"hidden_dim\": Hidden_dim,\n",
        "    \"final_dropout\": Final_dropout,\n",
        "    \"graph_pooling_type\": Graph_Pooling_Type,\n",
        "    \"neighbor_pooling_type\": Neighbor_Pooling_Type,\n",
        "    \"learn_eps\": Learn_Epsilon,\n",
        "    'degree_as_tag': Degree_as_tag,\n",
        "    'filename': File_Name\n",
        "    \n",
        "})\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYXLOV7M362q",
        "colab_type": "text"
      },
      "source": [
        "# Generating our graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HUMq47BF38iP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import networkx as nx\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "class S2VGraph(object):\n",
        "    def __init__(self, g, label, node_tags=None, node_features=None):\n",
        "        '''\n",
        "            g: a networkx graph\n",
        "            label: an integer graph label\n",
        "            node_tags: a list of integer node tags\n",
        "            node_features: a torch float tensor, one-hot representation of the tag that is used as input to neural nets\n",
        "            edge_mat: a torch long tensor, contain edge list, will be used to create torch sparse tensor\n",
        "            neighbors: list of neighbors (without self-loop)\n",
        "        '''\n",
        "        self.label = label\n",
        "        self.g = g\n",
        "        self.node_tags = node_tags\n",
        "        self.neighbors = []\n",
        "        self.node_features = 0\n",
        "        self.edge_mat = 0\n",
        "\n",
        "        self.max_neighbor = 0\n",
        "\n",
        "\n",
        "def load_data(dataset, degree_as_tag):\n",
        "    '''\n",
        "        dataset: name of dataset\n",
        "        test_proportion: ratio of test train split\n",
        "        seed: random seed for random splitting of dataset\n",
        "    '''\n",
        "\n",
        "    print('loading data')\n",
        "    g_list = []\n",
        "    label_dict = {}\n",
        "    feat_dict = {}\n",
        "    triggered = False\n",
        "\n",
        "    \n",
        "    #This 1st section of the load_data function reads every row in the text file.\n",
        "    #The first row of every graph will provide you with 2 things, the number of nodes, the associated label\n",
        "    #The subsequent rows represent each node, and will begin with 0\n",
        "    #Represented in this order (0,number of connections, <nodes that they are connected to>)\n",
        "    #The graphs are created to store each node, and to add the edge lists inside\n",
        "    #Thereafter, each graph is stored into a list known as g_list\n",
        "    with open('dataset/%s/%s.txt' % (dataset, dataset), 'r') as f:\n",
        "        n_g = int(f.readline().strip())\n",
        "        for i in range(n_g):\n",
        "            row = f.readline().strip().split()\n",
        "            n, l = [int(w) for w in row]\n",
        "            if not l in label_dict:\n",
        "                mapped = len(label_dict)\n",
        "                label_dict[l] = mapped\n",
        "            g = nx.Graph()\n",
        "            node_tags = []\n",
        "            node_features = []\n",
        "            n_edges = 0\n",
        "            for j in range(n):\n",
        "                g.add_node(j)\n",
        "                row = f.readline().strip().split()\n",
        "                tmp = int(row[1]) + 2\n",
        "                if tmp == len(row):\n",
        "                    # no node attributes\n",
        "                    row = [int(w) for w in row]\n",
        "                    attr = None\n",
        "                else:\n",
        "                    row, attr = [int(w) for w in row[:tmp]], np.array([float(w) for w in row[tmp:]])\n",
        "                    triggered = True\n",
        "                if triggered:\n",
        "                  print('Triggered already')\n",
        "                if not row[0] in feat_dict:\n",
        "                    mapped = len(feat_dict)\n",
        "                    feat_dict[row[0]] = mapped\n",
        "                node_tags.append(feat_dict[row[0]])\n",
        "\n",
        "                if tmp > len(row):\n",
        "                    node_features.append(attr)\n",
        "\n",
        "                n_edges += row[1]\n",
        "                for k in range(2, len(row)):\n",
        "                    g.add_edge(j, row[k])\n",
        "\n",
        "            if node_features != []:\n",
        "                node_features = np.stack(node_features)\n",
        "                node_feature_flag = True\n",
        "            else:\n",
        "                node_features = None\n",
        "                node_feature_flag = False\n",
        "\n",
        "            assert len(g) == n\n",
        "            g_list.append(S2VGraph(g, l, node_tags))\n",
        "\n",
        "            \n",
        "    #This is the 2nd part of the load_data code.\n",
        "    #Within each graph class, he is creating the following:\n",
        "    #1. graph.neighbors <matrix> : This is similar to the matrix A, but it only contains the data of the connections.\n",
        "    #2. graph.max_neighbor <int>: This is basically the maximum number of connections that arises from a single node in the graph\n",
        "    #3. graph.label <int>        : This is to ensure that the labels do not skip numbers, i.e. labels <1,2,4,5> becomes <0,1,2,3>\n",
        "    #4. graph.edge_mat <matrix>  : This is a transpose of the edge matrix from the graph [[nodes][connected nodes]]\n",
        "            \n",
        "    #add labels and edge_mat       \n",
        "    for g in g_list:\n",
        "        g.neighbors = [[] for i in range(len(g.g))]\n",
        "        for i, j in g.g.edges():\n",
        "            g.neighbors[i].append(j)\n",
        "            g.neighbors[j].append(i)\n",
        "            \n",
        "        degree_list = []\n",
        "        for i in range(len(g.g)):\n",
        "            g.neighbors[i] = g.neighbors[i]  #This line appears to be redundant\n",
        "            degree_list.append(len(g.neighbors[i]))         \n",
        "        g.max_neighbor = max(degree_list)\n",
        "        g.label = label_dict[g.label]  #All this line does, is ensure that there is no skipping of labels. The actual numerical label is just a variable.\n",
        "    \n",
        "        #These two lines create an extension of the edges to ensure that they are bi-directional\n",
        "        edges = [list(pair) for pair in g.g.edges()]\n",
        "        edges.extend([[i, j] for j, i in edges])\n",
        "\n",
        "        deg_list = list(dict(g.g.degree(range(len(g.g)))).values())\n",
        "        \n",
        "        g.edge_mat = tf.transpose(tf.constant(edges))\n",
        "        \n",
        "        \n",
        "        \n",
        "    #This part gives the degree dictionary but not in the order of nodes within the dataset.txt file  \n",
        "    if degree_as_tag:\n",
        "        for g in g_list:\n",
        "            g.node_tags = list(dict(g.g.degree).values())\n",
        "\n",
        "    #Extracting unique tag labels   \n",
        "    tagset = set([])\n",
        "    for g in g_list:\n",
        "        tagset = tagset.union(set(g.node_tags))\n",
        "    tagset = list(tagset)\n",
        "    tag2index = {tagset[i]:i for i in range(len(tagset))}\n",
        "    \n",
        "    \n",
        "    for g in g_list:\n",
        "        \n",
        "        node_features = np.zeros((len(g.node_tags), len(tagset)))\n",
        "        node_features[range(len(g.node_tags)), [tag2index[tag] for tag in g.node_tags]] = 1  \n",
        "        g.node_features = tf.constant(node_features)\n",
        "        \n",
        "        \n",
        "    print('# classes: %d' % len(label_dict))\n",
        "    print('# maximum node tag: %d' % len(tagset))\n",
        "\n",
        "    print(\"# data: %d\" % len(g_list))\n",
        "\n",
        "    return g_list, len(label_dict)\n",
        "\n",
        "def separate_data(graph_list, seed, fold_idx):\n",
        "    assert 0 <= fold_idx and fold_idx < 10, \"fold_idx must be from 0 to 9.\"\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle = True, random_state = seed)\n",
        "\n",
        "    labels = [graph.label for graph in graph_list]\n",
        "    idx_list = []\n",
        "    for idx in skf.split(np.zeros(len(labels)), labels):\n",
        "        idx_list.append(idx)\n",
        "    \n",
        "    train_idx, test_idx = idx_list[fold_idx]\n",
        "    \n",
        "    train_graph_list = [graph_list[i] for i in train_idx]\n",
        "    test_graph_list = [graph_list[i] for i in test_idx]\n",
        "\n",
        "    return train_graph_list, test_graph_list\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdXoYk8D3WjD",
        "colab_type": "text"
      },
      "source": [
        "# Defining our MLP Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-NQBL83a3WNk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class MLP(tf.keras.layers.Layer):\n",
        "  def __init__(self,num_layers, hidden_dim, output_dim):\n",
        "    '''\n",
        "        num_layers: number of layers in the neural networks (EXCLUDING the input layer). If num_layers=1, this reduces to linear model.\n",
        "        hidden_dim: dimensionality of hidden units at ALL layers\n",
        "        output_dim: number of classes for prediction\n",
        "    '''    \n",
        "    super(MLP,self).__init__()\n",
        "    \n",
        "    self.linear_or_not = True #default is linear model\n",
        "    self.num_layers = num_layers\n",
        "    \n",
        "    if num_layers < 1:\n",
        "        raise ValueError(\"number of layers should be positive!\")\n",
        "    elif num_layers == 1:\n",
        "        #Linear model\n",
        "        self.linear = Linear_model(output_dim = output_dim)\n",
        "    else:\n",
        "        #Multi-layer model\n",
        "        self.linear_or_not = False\n",
        "        self.multi = Multi_model(layers = num_layers,hidden_dim = hidden_dim,output_dim = output_dim)\n",
        "                     \n",
        "  def call(self,input_features):\n",
        "    if self.linear_or_not:\n",
        "        #If linear model\n",
        "        return self.linear(input_features)\n",
        "    else:\n",
        "        #If MLP\n",
        "        return self.multi(input_features)\n",
        "    \n",
        "  \n",
        "class Linear_model(tf.keras.layers.Layer):\n",
        "  def __init__(self, output_dim):\n",
        "    super(Linear_model,self).__init__()\n",
        "    self.output_layer = tf.keras.layers.Dense(units = output_dim)\n",
        "    \n",
        "  def call(self,input_features):\n",
        "    return self.output_layer(input_features)  \n",
        "    \n",
        "  \n",
        "class Multi_model(tf.keras.layers.Layer):\n",
        "  def __init__(self,layers,hidden_dim,output_dim):\n",
        "    super(Multi_model,self).__init__()\n",
        "    self.layers = layers\n",
        "    self.dense_list = []\n",
        "    self.batch_list = []\n",
        "    \n",
        "    for i in range(layers-1):\n",
        "      self.dense_list.append(tf.keras.layers.Dense(units = hidden_dim))\n",
        "      self.batch_list.append(tf.keras.layers.BatchNormalization())\n",
        "    self.dense_list.append(tf.keras.layers.Dense(units = output_dim))\n",
        "    \n",
        "  def call(self,input_features):\n",
        "    for i in range(self.layers-1):\n",
        "      densed = self.dense_list[i](input_features)\n",
        "      batched = self.batch_list[i](densed)\n",
        "      input_features = tf.nn.relu(batched)\n",
        "    multi_result = self.dense_list[-1](input_features)\n",
        "  \n",
        "    return multi_result \n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NNWMsDx3jci",
        "colab_type": "text"
      },
      "source": [
        "# Defining our GraphCNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6S1IFKfG3mEx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class GraphCNN(tf.keras.Model):\n",
        "    def __init__(self, num_layers, num_mlp_layers, hidden_dim, output_dim, final_dropout, learn_eps, graph_pooling_type, neighbor_pooling_type):\n",
        "        '''\n",
        "            num_layers: number of layers in the neural networks (INCLUDING the input layer)\n",
        "            num_mlp_layers: number of layers in mlps (EXCLUDING the input layer)\n",
        "            hidden_dim: dimensionality of hidden units at ALL layers\n",
        "            output_dim: number of classes for prediction\n",
        "            final_dropout: dropout ratio on the final linear layer\n",
        "            learn_eps: If True, learn epsilon to distinguish center nodes from neighboring nodes. If False, aggregate neighbors and center nodes altogether. \n",
        "            neighbor_pooling_type: how to aggregate neighbors (mean, average, or max)\n",
        "            graph_pooling_type: how to aggregate entire nodes in a graph (mean, average)\n",
        "        '''\n",
        "\n",
        "        super(GraphCNN, self).__init__()\n",
        "\n",
        "        self.final_dropout = final_dropout\n",
        "        self.num_layers = num_layers\n",
        "        self.graph_pooling_type = graph_pooling_type\n",
        "        self.neighbor_pooling_type = neighbor_pooling_type\n",
        "        self.learn_eps = learn_eps\n",
        "        self.eps = tf.Variable(tf.zeros(self.num_layers-1))\n",
        "\n",
        "    \n",
        "        ###List of MLPs\n",
        "        ###List of batchnorms applied to the output of MLP (input of the final prediction linear layer)\n",
        "        self.mlps = []\n",
        "        self.batches = []\n",
        "        self.linears = []\n",
        "        self.drops = []\n",
        "        \n",
        "        for layer in range(self.num_layers-1):\n",
        "            self.mlps.append(MLP(num_mlp_layers, hidden_dim, hidden_dim))\n",
        "            self.batches.append(tf.keras.layers.BatchNormalization())\n",
        "            self.linears.append(tf.keras.layers.Dense(output_dim))\n",
        "            self.drops.append(tf.keras.layers.Dropout(final_dropout))\n",
        "                    \n",
        "        self.linears.append(tf.keras.layers.Dense(output_dim))\n",
        "        self.drops.append(tf.keras.layers.Dropout(final_dropout))\n",
        "                \n",
        "    def __preprocess_neighbors_maxpool(self, batch_graph):\n",
        "        ###create padded_neighbor_list in concatenated graph\n",
        "        #compute the maximum number of neighbors within the graphs in the current minibatch\n",
        "        max_deg = max([graph.max_neighbor for graph in batch_graph])\n",
        "        padded_neighbor_list = []\n",
        "        start_idx = [0]\n",
        "\n",
        "        for i, graph in enumerate(batch_graph):\n",
        "            start_idx.append(start_idx[i] + len(graph.g))\n",
        "            padded_neighbors = []\n",
        "            for j in range(len(graph.neighbors)):\n",
        "                #add off-set values to the neighbor indices\n",
        "                pad = [n + start_idx[i] for n in graph.neighbors[j]]\n",
        "                #padding, dummy data is assumed to be stored in -1\n",
        "                pad.extend([-1]*(max_deg - len(pad)))\n",
        "\n",
        "                #Add center nodes in the maxpooling if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
        "                if not self.learn_eps:\n",
        "                    pad.append(j + start_idx[i])\n",
        "                \n",
        "                padded_neighbors.append(pad)\n",
        "            padded_neighbor_list.extend(padded_neighbors)        \n",
        "        return tf.constant(padded_neighbor_list)\n",
        "\n",
        "    def __preprocess_neighbors_sumavepool(self, batch_graph):\n",
        "        ###create block diagonal sparse matrix\n",
        "        edge_mat_list = []\n",
        "        start_idx = [0]\n",
        "        for i, graph in enumerate(batch_graph):\n",
        "            start_idx.append(start_idx[i] + len(graph.g))\n",
        "            edge_mat_list.append(graph.edge_mat + start_idx[i])\n",
        "        Adj_block_idx = tf.concat(edge_mat_list,1)\n",
        "        Adj_block_elem = tf.ones(Adj_block_idx.shape[1])\n",
        "        \n",
        "        #Add self-loops in the adjacency matrix if learn_eps is False, i.e., aggregate center nodes and neighbor nodes altogether.\n",
        "        if not self.learn_eps:\n",
        "            num_node = start_idx[-1]  #This is the number of nodes in the entire graph            \n",
        "            self_loop_edge = tf.constant([range(num_node),range(num_node)])\n",
        "            elem = tf.ones(num_node)\n",
        "            Adj_block_idx = tf.concat([Adj_block_idx, self_loop_edge], 1)   #Adding connections from self-connections to the list of other connections specified\n",
        "            Adj_block_elem = tf.concat([Adj_block_elem, elem], 0)  #Total number of connections + number of nodes\n",
        "            \n",
        "        Adj_block_idx = tf.cast(tf.transpose(Adj_block_idx),tf.int64)\n",
        "        Adj_block = tf.SparseTensor(indices=Adj_block_idx, values=Adj_block_elem, dense_shape=[start_idx[-1],start_idx[-1]])        \n",
        "        return Adj_block\n",
        "\n",
        "    def __preprocess_graphpool(self, batch_graph):\n",
        "        ###create sum or average pooling sparse matrix over entire nodes in each graph (num graphs x num nodes)        \n",
        "        start_idx = [0]\n",
        "        \n",
        "        #compute the padded neighbor list\n",
        "        for i, graph in enumerate(batch_graph):\n",
        "            start_idx.append(start_idx[i] + len(graph.g))\n",
        "\n",
        "        idx = []\n",
        "        elem = []\n",
        "        for i, graph in enumerate(batch_graph):\n",
        "            ###average pooling\n",
        "            if self.graph_pooling_type == \"average\":\n",
        "                elem.extend([1./len(graph.g)]*len(graph.g))\n",
        "            #By default, it goes to the else\n",
        "            else:\n",
        "            ###sum pooling\n",
        "                elem.extend([1]*len(graph.g))\n",
        "            idx.extend([[i, j] for j in range(start_idx[i], start_idx[i+1], 1)])  #idx will be [[0,0],[0,1]...[0,218<end of the 1st graph>],[1,219<end of the first graph + 1>]...]\n",
        "        \n",
        "        elem = tf.constant(elem)        \n",
        "        graph_pool = tf.SparseTensor(indices=idx, values=elem, dense_shape=[len(batch_graph),start_idx[-1]])\n",
        "        graph_pool = tf.cast(graph_pool,tf.float32)        #graph_pool is a diagonal matrix of ones, where the ones are rows corresponding to the length of each graph.\n",
        "        return graph_pool\n",
        "        \n",
        "    def maxpool(self, h, padded_neighbor_list):\n",
        "        ###Element-wise minimum will never affect max-pooling\n",
        "        dummy = tf.reduce_min(tf_output,axis=0,keepdims=True)\n",
        "        h_with_dummy = tf.concat([tf_output,tf_dummy],0)        \n",
        "        pooled_rep = tf.reduce_max(h_with_dummy[padded_neighbor_list], axis = 1)        \n",
        "        return pooled_rep\n",
        "\n",
        "\n",
        "    def next_layer_eps(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n",
        "        ###pooling neighboring nodes and center nodes separately by epsilon reweighting. \n",
        "\n",
        "        if self.neighbor_pooling_type == \"max\":\n",
        "            ##If max pooling\n",
        "            pooled = self.maxpool(h, padded_neighbor_list)\n",
        "        else:\n",
        "            #If sum or average pooling\n",
        "            h2 = tf.cast(h,tf.float32)\n",
        "            pooled = tf.sparse.sparse_dense_matmul(Adj_block,h2)\n",
        "\n",
        "            if self.neighbor_pooling_type == \"average\":  #The default is sum\n",
        "                #If average pooling                \n",
        "                degree = tf.sparse.sparse_dense_matmul(Adj_block,tf.ones([Adj_block.shape[0],1]))\n",
        "                pooled = pooled/degree\n",
        "       \n",
        "        #Reweights the center node representation when aggregating it with its neighbors\n",
        "        pooled = pooled + (1 + self.eps[layer])*h2        \n",
        "        pooled_rep = self.mlps[layer](pooled)\n",
        "                \n",
        "        h = self.batches[layer](pooled_rep)        \n",
        "        h = tf.nn.relu(h)\n",
        "        h = tf.cast(h,tf.float32)                \n",
        "        return h\n",
        "\n",
        "\n",
        "    def next_layer(self, h, layer, padded_neighbor_list = None, Adj_block = None):\n",
        "        ###pooling neighboring nodes and center nodes altogether  \n",
        "            \n",
        "        if self.neighbor_pooling_type == \"max\":\n",
        "            ##If max pooling\n",
        "            pooled = self.maxpool(h, padded_neighbor_list)\n",
        "        else:\n",
        "            #If sum or average pooling            \n",
        "            pooled = tf.sparse.sparse_dense_matmul(Adj_block,h)\n",
        "            if self.neighbor_pooling_type == \"average\":\n",
        "                #If average pooling\n",
        "                degree = tf.sparse.sparse_dense_matmul(Adj_block,tf.ones([Adj_block.shape[0],1]))                \n",
        "                pooled = pooled/degree\n",
        "\n",
        "        #representation of neighboring and center nodes        \n",
        "        pooled_rep = self.mlps[layer](pooled)\n",
        "        \n",
        "        h = self.batches[layer](pooled_rep)\n",
        "        \n",
        "        #non-linearity\n",
        "        h = tf.nn.relu(h)        \n",
        "        return h\n",
        "      \n",
        "    \n",
        "    def call(self, batch_graph):                \n",
        "        X_concat = tf.concat([graph.node_features for graph in batch_graph],0)        \n",
        "        graph_pool = self.__preprocess_graphpool(batch_graph)\n",
        "\n",
        "        if self.neighbor_pooling_type == \"max\":\n",
        "            padded_neighbor_list = self.__preprocess_neighbors_maxpool(batch_graph)\n",
        "        else:\n",
        "            Adj_block = self.__preprocess_neighbors_sumavepool(batch_graph)\n",
        "        \n",
        "        #list of hidden representation at each layer (including input)\n",
        "        hidden_rep = [X_concat]\n",
        "        h = X_concat\n",
        "                \n",
        "        for layer in range(self.num_layers-1):\n",
        "            if self.neighbor_pooling_type == \"max\" and self.learn_eps:\n",
        "                h = self.next_layer_eps(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
        "            elif not self.neighbor_pooling_type == \"max\" and self.learn_eps:  #This is the one that triggers for the reddit dataset\n",
        "                h = self.next_layer_eps(h, layer, Adj_block = Adj_block)\n",
        "            elif self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
        "                h = self.next_layer(h, layer, padded_neighbor_list = padded_neighbor_list)\n",
        "            elif not self.neighbor_pooling_type == \"max\" and not self.learn_eps:\n",
        "                h = self.next_layer(h, layer, Adj_block = Adj_block)        \n",
        "            hidden_rep.append(h)\n",
        "        score_over_layer = 0\n",
        "    \n",
        "        #perform pooling over all nodes in each graph in every layer\n",
        "        for layer, h in enumerate(hidden_rep):        \n",
        "            h = tf.cast(h,tf.float32)\n",
        "            pooled_h = tf.sparse.sparse_dense_matmul(graph_pool,h)            \n",
        "            linear_outcome = self.linears[layer](pooled_h)\n",
        "            dropped_outcome = self.drops[layer](linear_outcome)\n",
        "            score_over_layer += dropped_outcome\n",
        "\n",
        "        return score_over_layer  #This actually provides the value for the prediction      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O70ycCC64X6W",
        "colab_type": "text"
      },
      "source": [
        "# Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MgrJV0U4b_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "graphs, num_classes = load_data(args.dataset, args.degree_as_tag)\n",
        "train_graphs, test_graphs = separate_data(graphs, args.seed, args.fold_idx)\n",
        "labels = tf.constant([graph.label for graph in train_graphs])\n",
        "\n",
        "model = GraphCNN(args.num_layers, args.num_mlp_layers, args.hidden_dim, num_classes, args.final_dropout, args.learn_eps, args.graph_pooling_type, args.neighbor_pooling_type)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(lr = args.lr)\n",
        "\n",
        "#def train(loss,model,opt,original):    \n",
        "def train(args,model,train_graphs,opt,epoch):\n",
        "  total_iters = args.iters_per_epoch\n",
        "  pbar = tqdm(range(total_iters),unit = 'batch')\n",
        "  \n",
        "  loss_accum = 0\n",
        "  for pos in pbar:\n",
        "    selected_idx = np.random.permutation(len(train_graphs))[:args.batch_size]\n",
        "    batch_graph = [train_graphs[idx] for idx in selected_idx]\n",
        "    labels = tf.constant([graph.label for graph in batch_graph])\n",
        "    loss_accum = 0\n",
        "    with tf.GradientTape() as tape:\n",
        "      output = model(batch_graph)\n",
        "\n",
        "      loss = loss_object(labels,output)\n",
        "\n",
        "      \n",
        "    gradients = tape.gradient(loss,model.trainable_variables)\n",
        "    gradient_variables = zip(gradients, model.trainable_variables)\n",
        "    opt.apply_gradients(gradient_variables)\n",
        "    loss_accum += loss\n",
        "    \n",
        "    #report\n",
        "    pbar.set_description(f'epoch: {epoch}')\n",
        "    \n",
        "  average_loss = loss_accum/total_iters\n",
        "  print(f'loss training: {average_loss}')\n",
        "  return average_loss\n",
        "\n",
        "\n",
        "###pass data to model with minibatch during testing to avoid memory overflow (does not perform backpropagation)\n",
        "def pass_data_iteratively(model, graphs, minibatch_size = 64):\n",
        "    output = []\n",
        "    idx = np.arange(len(graphs))\n",
        "    for i in range(0, len(graphs), minibatch_size):\n",
        "        sampled_idx = idx[i:i+minibatch_size]\n",
        "        if len(sampled_idx) == 0:\n",
        "            continue    \n",
        "        output.append(model([graphs[j] for j in sampled_idx]))\n",
        "    return tf.concat(output,0)\n",
        "\n",
        "  \n",
        "def tf_check_acc(pred,labels):\n",
        "    pred = tf.cast(pred,tf.int32)\n",
        "    correct = tf.equal(pred,labels)\n",
        "    answer = 0\n",
        "    for element in correct:\n",
        "      if element:\n",
        "        answer +=1\n",
        "    return answer\n",
        "  \n",
        "def test(args, model, train_graphs, test_graphs, epoch):\n",
        "    output = pass_data_iteratively(model, train_graphs)\n",
        "    pred = tf.argmax(output,1)\n",
        "    labels = tf.constant([graph.label for graph in train_graphs])\n",
        "\n",
        "    correct = tf_check_acc(pred,labels)\n",
        "    acc_train = correct / float(len(train_graphs))\n",
        "\n",
        "    output = pass_data_iteratively(model, test_graphs)\n",
        "    pred = tf.argmax(output,1)\n",
        "    labels = tf.constant([graph.label for graph in test_graphs])\n",
        "    correct = tf_check_acc(pred,labels)\n",
        "    acc_test = correct / float(len(test_graphs))\n",
        "\n",
        "    print(\"accuracy train: %f test: %f\" % (acc_train, acc_test))\n",
        "\n",
        "    return acc_train, acc_test\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "    if epoch % 50 == 0:\n",
        "      optimizer.lr = optimizer.lr * 0.5    \n",
        "    print (optimizer.lr)\n",
        "    avg_loss = train(args, model, train_graphs, optimizer, epoch)\n",
        "    acc_train, acc_test = test(args, model, train_graphs, test_graphs, epoch)\n",
        "    \n",
        "    if not args.filename == \"\":\n",
        "        with open(args.filename, 'w') as f:\n",
        "            f.write(\"%f %f %f\" % (avg_loss, acc_train, acc_test))\n",
        "            f.write(\"\\n\")\n",
        "    print(\"\")\n",
        "\n",
        "    print(model.eps)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
